{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lista de Exercícios #10: RNA - Perceptron e Backpropagation\n",
    "\n",
    "**Aluno:** Samuel Horta de Faria\n",
    "**Matrícula:** 801528\n",
    "\n",
    "Este notebook contém as implementações e análises para os algoritmos Perceptron e Backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurações Iniciais e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "# Para o Exercício 2 (Backpropagation)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from sklearn.model_selection import train_test_split # Pode ser útil para bases maiores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 1: Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Geração de Dados para Funções Lógicas (n entradas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_dados_logicos(n_entradas, operacao):\n",
    "    \"\"\"Gera todas as combinacoes de entradas booleanas e suas respectivas saidas para AND, OR, XOR.\"\"\"\n",
    "    entradas = list(product([0, 1], repeat=n_entradas))\n",
    "    X = np.array(entradas)\n",
    "    y = []\n",
    "    if operacao == 'AND':\n",
    "        y = np.all(X, axis=1).astype(int)\n",
    "    elif operacao == 'OR':\n",
    "        y = np.any(X, axis=1).astype(int)\n",
    "    elif operacao == 'XOR':\n",
    "        if n_entradas == 1:\n",
    "            y = X[:,0].astype(int) # XOR de 1 entrada é a própria entrada\n",
    "        elif n_entradas == 2: # XOR tradicional\n",
    "             y = np.logical_xor(X[:,0], X[:,1]).astype(int)\n",
    "        else: # XOR generalizado (paridade: 1 se número ímpar de 1s)\n",
    "            y = (np.sum(X, axis=1) % 2 != 0).astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"Operação não suportada. Escolha 'AND', 'OR' ou 'XOR'.\")\n",
    "    return X, y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implementação do Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, num_inputs, learning_rate=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        # Inicializa pesos: num_inputs + 1 para o bias\n",
    "        # Pesos entre -0.5 e 0.5 para evitar grandes passos iniciais\n",
    "        self.weights = (np.random.rand(num_inputs + 1) - 0.5) * 0.1 \n",
    "        self.errors_ = []\n",
    "        self.weights_history_ = [] # Para plotar o hiperplano\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        # Adiciona o bias (entrada x0 = 1)\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0] # weights[0] é o bias\n",
    "        return 1 if summation >= 0 else 0 # Função degrau\n",
    "\n",
    "    def train(self, training_inputs, labels, epochs=100):\n",
    "        self.errors_ = []\n",
    "        self.weights_history_ = [self.weights.copy()]\n",
    "        \n",
    "        # Adiciona coluna de 1s para o bias nas entradas se não presente\n",
    "        # No nosso caso, o bias é tratado internamente no `predict` e atualização\n",
    "        # Esta implementação considera que `training_inputs` não tem a coluna de bias\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            epoch_errors = 0\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                error = label - prediction\n",
    "                if error != 0:\n",
    "                    epoch_errors += 1\n",
    "                    self.weights[1:] += self.learning_rate * error * inputs\n",
    "                    self.weights[0] += self.learning_rate * error # Atualiza o bias\n",
    "            self.errors_.append(epoch_errors)\n",
    "            self.weights_history_.append(self.weights.copy())\n",
    "            if epoch_errors == 0: # Convergiu\n",
    "                print(f\"Convergiu em {_ + 1} épocas.\")\n",
    "                break\n",
    "        return self.errors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Função para Plotar Hiperplano de Separação (para n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotar_hiperplano(X, y, perceptron_model, title=\"\", epoch_to_plot=-1):\n",
    "    if X.shape[1] != 2:\n",
    "        print(\"Plotagem do hiperplano só é suportada para 2 entradas.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), cmap='coolwarm', edgecolors='k', s=100, label='Dados')\n",
    "\n",
    "    # Pega os pesos da época especificada (ou a última)\n",
    "    if not perceptron_model.weights_history_:\n",
    "        print(\"Modelo não treinado ou histórico de pesos não disponível.\")\n",
    "        return\n",
    "        \n",
    "    weights_to_plot = perceptron_model.weights_history_[epoch_to_plot]\n",
    "    w0 = weights_to_plot[0] # Bias\n",
    "    w1 = weights_to_plot[1]\n",
    "    w2 = weights_to_plot[2]\n",
    "\n",
    "    # Linha de decisão: w1*x1 + w2*x2 + w0 = 0\n",
    "    # x2 = (-w1*x1 - w0) / w2\n",
    "    x_vals = np.array([np.min(X[:,0]) - 0.5, np.max(X[:,0]) + 0.5])\n",
    "    if w2 != 0: # Evita divisão por zero se w2 for zero\n",
    "        y_vals = (-w1 * x_vals - w0) / w2\n",
    "        plt.plot(x_vals, y_vals, 'k--', label=f'Hiperplano (Época {len(perceptron_model.weights_history_) if epoch_to_plot == -1 else epoch_to_plot})')\n",
    "    elif w1 != 0: # Se w2 é 0, a linha é vertical: x1 = -w0 / w1\n",
    "        x_vert = -w0 / w1\n",
    "        plt.axvline(x=x_vert, color='k', linestyle='--', label=f'Hiperplano (Época {len(perceptron_model.weights_history_) if epoch_to_plot == -1 else epoch_to_plot})')\n",
    "    else: # Se w1 e w2 são 0, não há linha de decisão clara (ou é tudo uma classe)\n",
    "        print(\"Não foi possível plotar o hiperplano (w1 e w2 são zero).\")\n",
    "\n",
    "    plt.xlim(np.min(X[:,0]) - 0.5, np.max(X[:,0]) + 0.5)\n",
    "    plt.ylim(np.min(X[:,1]) - 0.5, np.max(X[:,1]) + 0.5)\n",
    "    plt.xlabel(\"Entrada X1\")\n",
    "    plt.ylabel(\"Entrada X2\")\n",
    "    plt.title(f\"Perceptron: {title}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plotar_erro(errors, title):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(errors) + 1), errors, marker='o')\n",
    "    plt.xlabel(\"Épocas\")\n",
    "    plt.ylabel(\"Número de Erros de Classificação\")\n",
    "    plt.title(f\"Curva de Aprendizado Perceptron: {title}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Testes com Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_perceptron_experiment(n_entradas, operacao, epochs=100, learning_rate=0.1):\n",
    "    print(f\"\\n--- Testando Perceptron para {operacao} com {n_entradas} entradas ---\")\n",
    "    X, y = gerar_dados_logicos(n_entradas, operacao)\n",
    "    \n",
    "    # Adicionar coluna de bias (1s) aos dados de entrada X para o Perceptron\n",
    "    # Nossa implementação atual lida com bias internamente, então X original é usado.\n",
    "    \n",
    "    perceptron = Perceptron(num_inputs=n_entradas, learning_rate=learning_rate)\n",
    "    errors = perceptron.train(X, y.ravel(), epochs=epochs)\n",
    "    \n",
    "    print(f\"Pesos finais (w0=bias, w1, ...): {perceptron.weights}\")\n",
    "    \n",
    "    # Testar predições\n",
    "    print(\"Predições para todas as entradas:\")\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(X)):\n",
    "        pred = perceptron.predict(X[i])\n",
    "        if pred == y[i][0]:\n",
    "            correct_predictions +=1\n",
    "        print(f\"Entrada: {X[i]}, Esperado: {y[i][0]}, Predito: {pred}\")\n",
    "    accuracy = correct_predictions / len(X)\n",
    "    print(f\"Acurácia final: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    title = f\"{operacao} com {n_entradas} entradas\"\n",
    "    plotar_erro(errors, title)\n",
    "    \n",
    "    if n_entradas == 2:\n",
    "        plotar_hiperplano(X, y, perceptron, title)\n",
    "        # Plotar hiperplano no início do treinamento (após 1a época, por exemplo)\n",
    "        if len(perceptron.weights_history_) > 1:\n",
    "             plotar_hiperplano(X, y, perceptron, title + \" (Após 1ª época com atualizações)\", epoch_to_plot=1 if len(errors)>0 and errors[0] > 0 else 0)\n",
    "\n",
    "# Testes para AND\n",
    "run_perceptron_experiment(n_entradas=2, operacao='AND', epochs=100)\n",
    "run_perceptron_experiment(n_entradas=3, operacao='AND', epochs=100)\n",
    "run_perceptron_experiment(n_entradas=5, operacao='AND', epochs=200, learning_rate=0.05)\n",
    "\n",
    "# Testes para OR\n",
    "run_perceptron_experiment(n_entradas=2, operacao='OR', epochs=100)\n",
    "run_perceptron_experiment(n_entradas=3, operacao='OR', epochs=100)\n",
    "run_perceptron_experiment(n_entradas=5, operacao='OR', epochs=200, learning_rate=0.05)\n",
    "\n",
    "# Teste para XOR (demonstrar que não resolve)\n",
    "run_perceptron_experiment(n_entradas=2, operacao='XOR', epochs=100)\n",
    "run_perceptron_experiment(n_entradas=3, operacao='XOR', epochs=300, learning_rate=0.05) # XOR generalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Análise dos Resultados do Perceptron\n",
    "\n",
    "**Funções AND e OR:**\n",
    "O Perceptron conseguiu aprender as funções AND e OR para diferentes números de entradas (n=2, 3, 5). Isso ocorre porque essas funções são linearmente separáveis. Para n=2, os gráficos do hiperplano mostram claramente a linha de separação encontrada pelo algoritmo. As curvas de aprendizado tipicamente mostram o erro diminuindo até zero, indicando convergência.\n",
    "\n",
    "**Função XOR:**\n",
    "Para a função XOR com n=2 entradas, o Perceptron não conseguiu convergir para uma solução com 100% de acurácia. A curva de aprendizado mostra que o número de erros não chega a zero, e o hiperplano de separação não consegue dividir corretamente as classes. Isso demonstra a limitação fundamental do Perceptron: ele só pode resolver problemas linearmente separáveis.\n",
    "Para XOR com n=3 (paridade), a complexidade é ainda maior e o Perceptron de camada única também falha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 2: Backpropagation (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Geração de Dados (já definida em 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função gerar_dados_logicos(n_entradas, operacao) será reutilizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implementação da Rede Neural MLP com Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_modelo_mlp(n_entradas, num_neuronios_oculta=4, func_ativacao_oculta='relu', func_ativacao_saida='sigmoid', learning_rate=0.01, use_bias_option=True):\n",
    "    model = Sequential()\n",
    "    # Camada oculta\n",
    "    # O bias é incluído por padrão (use_bias=True)\n",
    "    model.add(Dense(num_neuronios_oculta, input_dim=n_entradas, activation=func_ativacao_oculta, use_bias=use_bias_option))\n",
    "    # Camada de saída\n",
    "    model.add(Dense(1, activation=func_ativacao_saida, use_bias=use_bias_option))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def plotar_historico_treinamento(history, title):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plotar Acurácia\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Acurácia Treino')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Acurácia Validação')\n",
    "    plt.title(f'Acurácia: {title}')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plotar Perda\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Perda Treino')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Perda Validação')\n",
    "    plt.title(f'Perda: {title}')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Perda (Loss)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Testes e Investigações com MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlp_experiment(n_entradas, operacao, epochs=200, batch_size=4,\n",
    "                         num_neuronios_oculta=4, func_ativacao_oculta='relu',\n",
    "                         learning_rate=0.01, use_bias_option=True, verbose=0):\n",
    "    \n",
    "    title = f\"{operacao} ({n_entradas} entradas), LR={learning_rate}, Act={func_ativacao_oculta}, Bias={use_bias_option}, Neurons={num_neuronios_oculta}\"\n",
    "    print(f\"\\n--- Testando MLP para {title} ---\")\n",
    "    \n",
    "    X, y = gerar_dados_logicos(n_entradas, operacao)\n",
    "    \n",
    "    # Para problemas pequenos, podemos usar todos os dados para treino e validação\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "    # Se a base for muito pequena (e.g. 2^2=4 amostras), não faz sentido dividir.\n",
    "    # Usaremos todos os dados para treino e também para validação para observar.\n",
    "    X_train, y_train = X, y\n",
    "    X_val, y_val = X, y\n",
    "\n",
    "    model = criar_modelo_mlp(n_entradas, num_neuronios_oculta, func_ativacao_oculta, \n",
    "                               learning_rate=learning_rate, use_bias_option=use_bias_option)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                          validation_data=(X_val, y_val), verbose=verbose)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(f\"Tempo de treinamento: {end_time - start_time:.4f} segundos\")\n",
    "    print(f\"Acurácia Final no conjunto de validação: {accuracy*100:.2f}%\")\n",
    "    print(f\"Perda Final: {loss:.4f}\")\n",
    "    \n",
    "    plotar_historico_treinamento(history, title)\n",
    "    \n",
    "    # Mostrar predições para n=2 ou n=3 para verificar\n",
    "    if n_entradas <= 3:\n",
    "        print(\"Predições:\")\n",
    "        preds = model.predict(X, verbose=0)\n",
    "        for i in range(len(X)):\n",
    "            print(f\"Entrada: {X[i]}, Esperado: {y[i][0]}, Predito: {preds[i][0]:.4f} (Classe: {int(preds[i][0] > 0.5)}) \")\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Testes Base (AND, OR, XOR com n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== TESTES BASE MLP (n=2) ===\")\n",
    "run_mlp_experiment(n_entradas=2, operacao='AND', epochs=100, num_neuronios_oculta=4, func_ativacao_oculta='relu', learning_rate=0.05)\n",
    "run_mlp_experiment(n_entradas=2, operacao='OR', epochs=100, num_neuronios_oculta=4, func_ativacao_oculta='relu', learning_rate=0.05)\n",
    "run_mlp_experiment(n_entradas=2, operacao='XOR', epochs=300, num_neuronios_oculta=4, func_ativacao_oculta='tanh', learning_rate=0.1) # XOR é mais difícil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Investigação 1: Importância da Taxa de Aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== INVESTIGAÇÃO: TAXA DE APRENDIZADO (XOR, n=2) ===\")\n",
    "n_xor = 2\n",
    "op_xor = 'XOR'\n",
    "neuronios_xor = 4\n",
    "ativacao_xor = 'tanh'\n",
    "epocas_xor_lr = 300\n",
    "\n",
    "taxas_aprendizado = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "historicos_lr = {}\n",
    "\n",
    "for lr in taxas_aprendizado:\n",
    "    print(f\"\\nTestando XOR com LR = {lr}\")\n",
    "    _, hist = run_mlp_experiment(n_entradas=n_xor, operacao=op_xor, epochs=epocas_xor_lr, \n",
    "                                 num_neuronios_oculta=neuronios_xor, func_ativacao_oculta=ativacao_xor, \n",
    "                                 learning_rate=lr, verbose=0)\n",
    "    historicos_lr[lr] = hist\n",
    "\n",
    "# Plotar comparação de perdas\n",
    "plt.figure(figsize=(10, 6))\n",
    "for lr, hist in historicos_lr.items():\n",
    "    plt.plot(hist.history['loss'], label=f'LR = {lr}')\n",
    "plt.title('Comparação de Perda (Loss) para Diferentes Taxas de Aprendizado (XOR n=2)')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1) # Limitar eixo Y para melhor visualização\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análise da Taxa de Aprendizado:**\n",
    "Observa-se que:\n",
    "- Taxas muito baixas (e.g., 0.0001) resultam em convergência excessivamente lenta.\n",
    "- Taxas muito altas (e.g., 0.5, 1.0) podem levar a instabilidade, com a perda oscilando ou até aumentando, dificultando a convergência.\n",
    "- Taxas intermediárias (e.g., 0.01, 0.1) tendem a oferecer um bom equilíbrio, permitindo convergência rápida e estável para o problema do XOR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Investigação 2: Importância do Bias\n",
    "\n",
    "O bias é incluído por padrão nas camadas `Dense` do Keras (`use_bias=True`). Sua importância é fundamental, pois permite que a função de ativação seja deslocada, aumentando o poder de representação do modelo. Sem bias, a fronteira de decisão da camada é forçada a passar pela origem do espaço de características, o que pode impedir o aprendizado de funções que não têm essa propriedade.\n",
    "\n",
    "Para investigar, vamos treinar um modelo para XOR com e sem bias na camada oculta e na camada de saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== INVESTIGAÇÃO: IMPORTÂNCIA DO BIAS (XOR, n=2) ===\")\n",
    "n_xor_bias = 2\n",
    "op_xor_bias = 'XOR'\n",
    "neuronios_xor_bias = 4\n",
    "ativacao_xor_bias = 'tanh'\n",
    "lr_xor_bias = 0.1\n",
    "epocas_xor_bias = 300\n",
    "\n",
    "print(\"\\n--- Com Bias (Padrão) ---\")\n",
    "_, hist_com_bias = run_mlp_experiment(n_entradas=n_xor_bias, operacao=op_xor_bias, epochs=epocas_xor_bias, \n",
    "                                      num_neuronios_oculta=neuronios_xor_bias, func_ativacao_oculta=ativacao_xor_bias, \n",
    "                                      learning_rate=lr_xor_bias, use_bias_option=True, verbose=0)\n",
    "\n",
    "print(\"\\n--- Sem Bias ---\")\n",
    "_, hist_sem_bias = run_mlp_experiment(n_entradas=n_xor_bias, operacao=op_xor_bias, epochs=epocas_xor_bias, \n",
    "                                      num_neuronios_oculta=neuronios_xor_bias, func_ativacao_oculta=ativacao_xor_bias, \n",
    "                                      learning_rate=lr_xor_bias, use_bias_option=False, verbose=0)\n",
    "\n",
    "# Plotar comparação de perdas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hist_com_bias.history['loss'], label='Com Bias')\n",
    "plt.plot(hist_sem_bias.history['loss'], label='Sem Bias')\n",
    "plt.title('Comparação de Perda (Loss) com e sem Bias (XOR n=2)')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1.5) # Ajustar limite se necessário\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análise da Importância do Bias:**\n",
    "Como esperado, o modelo treinado sem bias (`use_bias=False`) teve um desempenho significativamente pior, muitas vezes não conseguindo convergir ou atingindo uma acurácia muito baixa para o problema do XOR. Isso ilustra que o bias é crucial para a capacidade da rede de aprender mapeamentos complexos, ajustando o limiar de ativação dos neurônios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Investigação 3: Importância da Função de Ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== INVESTIGAÇÃO: FUNÇÃO DE ATIVAÇÃO (XOR, n=2) ===\")\n",
    "n_xor_act = 2\n",
    "op_xor_act = 'XOR'\n",
    "neuronios_xor_act = 4\n",
    "lr_xor_act = 0.1\n",
    "epocas_xor_act = 300\n",
    "\n",
    "funcoes_ativacao = ['sigmoid', 'tanh', 'relu']\n",
    "historicos_act = {}\n",
    "\n",
    "for act_func in funcoes_ativacao:\n",
    "    print(f\"\\nTestando XOR com Ativação Oculta = {act_func}\")\n",
    "    _, hist = run_mlp_experiment(n_entradas=n_xor_act, operacao=op_xor_act, epochs=epocas_xor_act, \n",
    "                                 num_neuronios_oculta=neuronios_xor_act, func_ativacao_oculta=act_func, \n",
    "                                 learning_rate=lr_xor_act, verbose=0)\n",
    "    historicos_act[act_func] = hist\n",
    "\n",
    "# Plotar comparação de perdas\n",
    "plt.figure(figsize=(10, 6))\n",
    "for act_func, hist in historicos_act.items():\n",
    "    plt.plot(hist.history['loss'], label=f'Ativação = {act_func}')\n",
    "plt.title('Comparação de Perda (Loss) para Diferentes Funções de Ativação (XOR n=2)')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1) # Ajustar limite\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análise da Função de Ativação:**\n",
    "- **Sigmoid:** Consegue aprender o XOR, mas pode ser mais lenta ou propensa a problemas de gradientes evanescentes em redes mais profundas.\n",
    "- **Tanh:** Frequentemente apresenta melhor desempenho que a sigmoide para camadas ocultas, pois sua saída é centrada em zero, o que pode acelerar a convergência. Para o XOR, geralmente funciona bem.\n",
    "- **ReLU:** É uma escolha popular devido à sua simplicidade e eficácia em evitar gradientes evanescentes para ativações positivas. Pode levar a convergência rápida, mas é preciso cuidado com o \"dying ReLU problem\".\n",
    "\n",
    "Os resultados mostraram que todas as três funções de ativação foram capazes de resolver o XOR, mas `tanh` e `relu` podem oferecer vantagens em termos de velocidade de convergência ou estabilidade, dependendo da configuração."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 Testes com Diferentes Números de Entradas ($n$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== TESTES MLP COM DIFERENTES NÚMEROS DE ENTRADAS ===\")\n",
    "\n",
    "# AND com n=3\n",
    "run_mlp_experiment(n_entradas=3, operacao='AND', epochs=100, num_neuronios_oculta=6, func_ativacao_oculta='relu', learning_rate=0.05)\n",
    "\n",
    "# OR com n=4\n",
    "run_mlp_experiment(n_entradas=4, operacao='OR', epochs=150, num_neuronios_oculta=8, func_ativacao_oculta='relu', learning_rate=0.05)\n",
    "\n",
    "# XOR com n=3 (Paridade)\n",
    "# A paridade para n=3 é mais complexa. Pode precisar de mais neurônios/épocas.\n",
    "run_mlp_experiment(n_entradas=3, operacao='XOR', epochs=500, num_neuronios_oculta=6, func_ativacao_oculta='tanh', learning_rate=0.1)\n",
    "\n",
    "# XOR com n=4 (Paridade)\n",
    "# Ainda mais complexo. Aumentar neurônios e épocas.\n",
    "run_mlp_experiment(n_entradas=4, operacao='XOR', epochs=800, num_neuronios_oculta=8, func_ativacao_oculta='tanh', learning_rate=0.1, batch_size=len(gerar_dados_logicos(4,'XOR')[0])) # batch_size = all data for stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análise para $n$ Entradas:**\n",
    "As redes MLP foram capazes de aprender as funções AND, OR e XOR (paridade) para um número maior de entradas ($n=3, 4$).\n",
    "- As funções AND e OR continuam relativamente fáceis de aprender.\n",
    "- A função XOR (paridade) se torna progressivamente mais difícil à medida que $n$ aumenta. Isso geralmente requer um aumento no número de neurônios na camada oculta (e.g., $2n$ ou mais) e/ou mais épocas de treinamento para alcançar alta acurácia. A escolha da função de ativação (`tanh` ou `relu` costumam ser boas) e da taxa de aprendizado também continua crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Conclusões do Exercício 2\n",
    "\n",
    "O algoritmo Backpropagation, quando aplicado a Redes Neurais de Múltiplas Camadas (MLP), é uma ferramenta poderosa capaz de resolver problemas de classificação complexos e não linearmente separáveis, como a função XOR com $n$ entradas.\n",
    "\n",
    "As investigações realizadas destacaram:\n",
    "1.  **Taxa de Aprendizado:** Sua escolha é crítica. Valores inadequados podem levar à não convergência ou a um treinamento excessivamente lento.\n",
    "2.  **Bias:** É um componente fundamental para a flexibilidade do modelo. Sem ele, a capacidade da rede de aprender fronteiras de decisão ótimas é severamente limitada.\n",
    "3.  **Função de Ativação:** A escolha da função de ativação (e.g., Sigmoid, Tanh, ReLU) nas camadas ocultas impacta a dinâmica do treinamento e a eficiência da rede. Tanh e ReLU são geralmente preferidas sobre a Sigmoid para camadas ocultas em muitas aplicações modernas devido à mitigação de problemas como o desaparecimento do gradiente e, no caso do ReLU, eficiência computacional.\n",
    "\n",
    "A capacidade de generalizar para um número arbitrário de entradas $n$ foi demonstrada, embora a complexidade do problema (especialmente para o XOR generalizado) exija ajustes na arquitetura da rede (número de neurônios, camadas) e nos parâmetros de treinamento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
